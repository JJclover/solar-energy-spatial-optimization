{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import geopandas as gpd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SedonaApp\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.sedona:sedona-python-adapter-3.0_2.12:1.0.0-incubating\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryo.registrator\", \"org.apache.sedona.core.serde.SedonaKryoRegistrator\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set Sedona configuration\n",
    "spark.sparkContext.setSystemProperty(\"sedona.global.charset\", \"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.register import SedonaRegistrator\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.utils.adapter import Adapter\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "from sedona.core.SpatialRDD import SpatialRDD\n",
    "\n",
    "# Read a shapefile into a SpatialRDD\n",
    "spatial_rdd = ShapefileReader.readToGeometryRDD(spark.sparkContext, \"path_to_shapefile\")\n",
    "\n",
    "# Transform the SpatialRDD to a DataFrame\n",
    "spatial_df = Adapter.toDf(spatial_rdd, spark)\n",
    "\n",
    "# Register the DataFrame as a temporary table and run SQL queries\n",
    "spatial_df.createOrReplaceTempView(\"spatial_table\")\n",
    "result = spark.sql(\"SELECT * FROM spatial_table WHERE st_contains(geometry, st_point(1.5, 1.5))\")\n",
    "\n",
    "# Show the results\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"ConcatCSVFiles\").getOrCreate()\n",
    "\n",
    "# Folder where the CSV files are located\n",
    "folder_path = \"data\"\n",
    "\n",
    "# Define the range of months\n",
    "start_month = datetime.strptime(\"202204\", \"%Y%m\")\n",
    "end_month = datetime.strptime(\"202303\", \"%Y%m\")\n",
    "\n",
    "# Initialize an empty DataFrame to store final concatenated data\n",
    "consumption_malaga_df = None\n",
    "\n",
    "# Loop through each month in the range\n",
    "current_month = start_month\n",
    "while current_month <= end_month:\n",
    "    # Construct file names\n",
    "    file_suffix = current_month.strftime(\"%Y%m\")\n",
    "    file_consumos = os.path.join(folder_path, f\"{file_suffix}_SIPS2_CONSUMOS_ELECTRICIDAD_peninsular.csv\")\n",
    "    file_ps = os.path.join(folder_path, f\"{file_suffix}_SIPS2_PS_ELECTRICIDAD_peninsular.csv\")\n",
    "    \n",
    "    # Read the CSV files into Spark DataFrames\n",
    "    if os.path.exists(file_consumos) and os.path.exists(file_ps):\n",
    "        df_c = spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .csv(file_consumos)\n",
    "        \n",
    "        df_ps = spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .csv(file_ps)\n",
    "        \n",
    "        # Dropping columns in df_ps that are also present in df_c\n",
    "        columns_in_df_c = df_c.columns\n",
    "        for column in columns_in_df_c:\n",
    "            if column in df_ps.columns and column != 'cups':\n",
    "                df_ps = df_ps.drop(column)\n",
    "\n",
    "        # Join the filtered df_ps with df_c on the 'cups' column\n",
    "        df = df_ps.join(df_c, on='cups', how='inner')\n",
    "\n",
    "        # Filter the DataFrame where 'codigoProvinciaPS' is equal to 29\n",
    "        df_malaga = df.filter(df.codigoProvinciaPS == 29)\n",
    "        \n",
    "        # Concatenate the individual DataFrame with the final DataFrame\n",
    "        if final_df is None:\n",
    "            final_df = df_malaga\n",
    "        else:\n",
    "            final_df = final_df.union(df_malaga)\n",
    "        \n",
    "    else:\n",
    "        print(f\"The files for {file_suffix} were not found in the folder '{folder_path}'.\")\n",
    "\n",
    "    # Move to the next month\n",
    "    current_month = current_month + timedelta(days=31)\n",
    "    current_month = current_month.replace(day=1)\n",
    "\n",
    "# Now final_df contains the concatenated data. You can perform further operations on it.\n",
    "if consumption_malaga_df:\n",
    "    consumption_malaga_df.show()\n",
    "else:\n",
    "    print(\"No data to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
